{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "hn = pd.read_csv(\"hacker_news.csv\")\n",
    "titles = hn['title']\n",
    "\n",
    "pattern=r\"sql\"\n",
    "\n",
    "sql_counts=titles.str.contains(pattern,flags=re.I).sum()\n",
    "print(sql_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/dq-content/369/single_capture_group.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id  ...      flavor\n",
      "142    10957172  ...  postgresql\n",
      "221    11544342  ...      memsql\n",
      "882    10413272  ...  postgresql\n",
      "1160   10546681  ...       nosql\n",
      "1197   11583183  ...  postgresql\n",
      "1370   10532855  ...       nosql\n",
      "2430   12300670  ...       mysql\n",
      "2432   10361294  ...       nosql\n",
      "4546   11437660  ...  postgresql\n",
      "4568   10725042  ...       nosql\n",
      "4616   10674187  ...  postgresql\n",
      "4944   10519135  ...       nosql\n",
      "5398   12430768  ...  postgresql\n",
      "5523   11174174  ...       mysql\n",
      "5654   11170360  ...    sparksql\n",
      "5738   10484824  ...       mysql\n",
      "5844   11984351  ...       nosql\n",
      "6523   12576116  ...  postgresql\n",
      "6532   10469304  ...       mysql\n",
      "7050   12329499  ...       mysql\n",
      "7245   12142364  ...  postgresql\n",
      "7571   12576002  ...  postgresql\n",
      "8371   11588305  ...  postgresql\n",
      "8823   10761955  ...  postgresql\n",
      "9643   11353322  ...  postgresql\n",
      "10238  10204844  ...  postgresql\n",
      "10264  11183348  ...  postgresql\n",
      "10478  11458621  ...       nosql\n",
      "10851  11927626  ...       nosql\n",
      "11793  11284523  ...       nosql\n",
      "12757  11683306  ...  postgresql\n",
      "12766  12241157  ...       mysql\n",
      "12800  12542095  ...       nosql\n",
      "12973  12529310  ...       nosql\n",
      "14132  10652254  ...       nosql\n",
      "14527  10696058  ...       nosql\n",
      "15089  11089907  ...       mysql\n",
      "15348  10458138  ...       nosql\n",
      "15898  11608757  ...  postgresql\n",
      "16095  10319153  ...       mysql\n",
      "16099  12267150  ...       mysql\n",
      "16135  12289975  ...       nosql\n",
      "16239  10838945  ...  postgresql\n",
      "16688  10444175  ...  postgresql\n",
      "16925  12114429  ...       nosql\n",
      "17104  11340344  ...  postgresql\n",
      "17207  12023517  ...  postgresql\n",
      "17660  12464343  ...    cloudsql\n",
      "17983  10707569  ...       mysql\n",
      "17995  10810266  ...  postgresql\n",
      "18063  12203959  ...       mysql\n",
      "18072  11755358  ...       nosql\n",
      "18166  10923848  ...       mysql\n",
      "18666  11502251  ...  postgresql\n",
      "18828  12352831  ...  postgresql\n",
      "19066  12238169  ...       mysql\n",
      "19087  11662536  ...  postgresql\n",
      "19133  12041615  ...  postgresql\n",
      "19580  12252112  ...  postgresql\n",
      "19802  12223216  ...  postgresql\n",
      "\n",
      "[60 rows x 8 columns]\n",
      "            num_comments\n",
      "flavor                  \n",
      "cloudsql        5.000000\n",
      "memsql         14.000000\n",
      "mysql          12.230769\n",
      "nosql          14.529412\n",
      "postgresql     25.962963\n",
      "sparksql        1.000000\n"
     ]
    }
   ],
   "source": [
    "hn_sql = hn[hn['title'].str.contains(r\"\\w+SQL\", flags=re.I)].copy()\n",
    "\n",
    "hn_sql[\"flavor\"]=hn_sql[\"title\"].str.extract(r\"(\\w+sql)\",flags=re.I)\n",
    "\n",
    "hn_sql[\"flavor\"]=hn_sql[\"flavor\"].str.lower()\n",
    "\n",
    "sql_pivot=hn_sql.pivot_table(values=\"num_comments\",index=\"flavor\")\n",
    "\n",
    "print(hn_sql)\n",
    "\n",
    "print(sql_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using Capture Groups to Extract Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        NaN\n",
      "1        NaN\n",
      "2        NaN\n",
      "3        NaN\n",
      "4        NaN\n",
      "        ... \n",
      "20094    NaN\n",
      "20095    NaN\n",
      "20096    NaN\n",
      "20097    NaN\n",
      "20098    NaN\n",
      "Name: title, Length: 20099, dtype: object {'3': 10, '3.5': 3, '2': 3, '3.6': 2, '4': 1, '3.5.0': 1, '1.5': 1, '2.7': 1, '8': 1}\n"
     ]
    }
   ],
   "source": [
    "pattern=r\"[Pp]ython ([\\d.]+)\"\n",
    "\n",
    "py_versions=titles.str.extract(pattern,expand=False)\n",
    "\n",
    "py_versions_freq=dict(py_versions.value_counts())\n",
    "\n",
    "print(py_versions,py_versions_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Counting Mentions of the 'C' Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365                      The new C standards are worth it\n",
      "444           Moz raises $10m Series C from Foundry Group\n",
      "521          Fuchsia: Micro kernel written in C by Google\n",
      "1307            Show HN: Yupp, yet another C preprocessor\n",
      "1326                     The C standard formalized in Coq\n",
      "1365                          GNU C Library 2.23 released\n",
      "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
      "1620                        SDCC  Small Device C Compiler\n",
      "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
      "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def first_10_matches(pattern):\n",
    "    \"\"\"\n",
    "    Return the first 10 story titles that match\n",
    "    the provided regular expression\n",
    "    \"\"\"\n",
    "    all_matches = titles[titles.str.contains(pattern)]\n",
    "    first_10 = all_matches.head(10)\n",
    "    return first_10\n",
    "\n",
    "pattern = r\"\\b[Cc]\\b[^\\+\\.]\"\n",
    "\n",
    "first_ten=first_10_matches(pattern)\n",
    "\n",
    "print(first_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using Lookarounds to Control Matches Based on Surrounding Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/dq-content/369/lookarounds.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365                       The new C standards are worth it\n",
      "521           Fuchsia: Micro kernel written in C by Google\n",
      "1307             Show HN: Yupp, yet another C preprocessor\n",
      "1326                      The C standard formalized in Coq\n",
      "1365                           GNU C Library 2.23 released\n",
      "                               ...                        \n",
      "18543                 C-style for loops removed from Swift\n",
      "18549            Show HN: An awesome C library for Windows\n",
      "18649                 Python vs. C/C++ in embedded systems\n",
      "19151                      Ask HN: How to learn C in 2016?\n",
      "19933    Lightweight C library to parse NMEA 0183 sente...\n",
      "Name: title, Length: 102, dtype: object\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "pattern=r\"(?<!Series\\s)\\b[Cc]\\b(?![.\\+])\"\n",
    "\n",
    "\n",
    "print(titles[titles.str.contains(pattern)])\n",
    "\n",
    "c_mentions=titles.str.contains(pattern).sum()\n",
    "print(c_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. BackReferences: Using Capture Groups in a RegEx Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/dq-content/369/backreference_syntax_1.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3102                  Silicon Valley Has a Problem Problem\n",
      "3176                Wire Wire: A West African Cyber Threat\n",
      "3178                         Flexbox Cheatsheet Cheatsheet\n",
      "4797                            The Mindset Mindset (2015)\n",
      "7276     Valentine's Day Special: Bye Bye Tinder, Flirt...\n",
      "10371    Mcdonalds copying cyriak  cows cows cows in th...\n",
      "11575                                    Bang Bang Control\n",
      "11901          Cordless Telephones: Bye Bye Privacy (1991)\n",
      "12697          Solving the the Monty-Hall-Problem in Swift\n",
      "15049    Bye Bye Webrtc2SIP: WebRTC with Asterisk and A...\n",
      "15839          Intellij-Rust Rust Plugin for IntelliJ IDEA\n",
      "Name: title, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "pattern=r\"\\b(\\w+)\\s\\1\\b\"\n",
    "\n",
    "repeated_words=titles[titles.str.contains(pattern)]\n",
    "\n",
    "print(repeated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Substituting Regular Expression Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     email\n",
      "1     Email\n",
      "2    e Mail\n",
      "3    e mail\n",
      "4    E-mail\n",
      "5    e-mail\n",
      "6     eMail\n",
      "7    E-Mail\n",
      "8     EMAIL\n",
      "dtype: object\n",
      "0    email\n",
      "1    email\n",
      "2    email\n",
      "3    email\n",
      "4    email\n",
      "5    email\n",
      "6    email\n",
      "7    email\n",
      "8    email\n",
      "dtype: object\n",
      "0                                Interactive Dynamic Video\n",
      "1        Florida DJs May Face Felony for April Fools' W...\n",
      "2             Technology ventures: From Idea to Enterprise\n",
      "3        Note by Note: The Making of Steinway L1037 (2007)\n",
      "4        Title II kills investment? Comcast and other I...\n",
      "                               ...                        \n",
      "20094    How Purism Avoids Intels Active Management Tec...\n",
      "20095            YC Application Translated and Broken Down\n",
      "20096    Microkernels are slow and Elvis didn't do no d...\n",
      "20097                        How Product Hunt really works\n",
      "20098    RoboBrowser: Your friendly neighborhood web sc...\n",
      "Name: title, Length: 20099, dtype: object\n"
     ]
    }
   ],
   "source": [
    "email_variations = pd.Series(['email', 'Email', 'e Mail',\n",
    "                        'e mail', 'E-mail', 'e-mail',\n",
    "                        'eMail', 'E-Mail', 'EMAIL'])\n",
    "\n",
    "pattern=r\"\\be[\\s-]?mail?\"\n",
    "\n",
    "print(email_variations)\n",
    "\n",
    "email_uniform=email_variations.str.replace(pattern,\"email\",flags=re.I)\n",
    "\n",
    "print(email_uniform)\n",
    "\n",
    "titles_clean=titles.str.replace(pattern,\"email\",flags=re.I)\n",
    "\n",
    "print(titles_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extracting Domains from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      www.amazon.com\n",
      "1     www.interactivedynamicvideo.com\n",
      "2                     www.nytimes.com\n",
      "3                       evonomics.com\n",
      "4                          github.com\n",
      "5                            phys.org\n",
      "6                        iot.seeed.cc\n",
      "7                    www.bfilipek.com\n",
      "8               beta.crowdfireapp.com\n",
      "9                        www.valid.ly\n",
      "10          css-cursor.techstream.org\n",
      "dtype: object\n",
      "0        www.interactivedynamicvideo.com\n",
      "1                        www.thewire.com\n",
      "2                         www.amazon.com\n",
      "3                        www.nytimes.com\n",
      "4                        arstechnica.com\n",
      "                      ...               \n",
      "20094                            puri.sm\n",
      "20095                         medium.com\n",
      "20096                 blog.darknedgy.net\n",
      "20097                         medium.com\n",
      "20098                         github.com\n",
      "Name: url, Length: 20099, dtype: object\n",
      "github.com             1008\n",
      "medium.com              825\n",
      "www.nytimes.com         525\n",
      "www.theguardian.com     248\n",
      "techcrunch.com          245\n",
      "Name: url, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_urls = pd.Series([\n",
    " 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
    " 'http://www.interactivedynamicvideo.com/',\n",
    " 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
    " 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n",
    " 'HTTPS://github.com/keppel/pinn',\n",
    " 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n",
    " 'https://iot.seeed.cc',\n",
    " 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n",
    " 'http://beta.crowdfireapp.com/?beta=agnipath',\n",
    " 'https://www.valid.ly?param',\n",
    " 'http://css-cursor.techstream.org'\n",
    "])\n",
    "\n",
    "pattern = r\"https?://([\\w\\.\\-]+)\"\n",
    "\n",
    "test_urls_clean=test_urls.str.extract(pattern,flags=re.I,expand=False)\n",
    "\n",
    "print(test_urls_clean)\n",
    "\n",
    "domains=hn[\"url\"].str.extract(pattern, flags=re.I,expand=False)\n",
    "\n",
    "print(domains)\n",
    "\n",
    "top_domains=domains.value_counts().head()\n",
    "\n",
    "print(top_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Extracting URL Parts Using Multiple Capture Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0  ...                                                  2\n",
      "0   https  ...  Technology-Ventures-Enterprise-Thomas-Byers/dp...\n",
      "1    http  ...                                                   \n",
      "2    http  ...                2007/11/07/movies/07stein.html?_r=0\n",
      "3    http  ...  advertising-cannot-maintain-internet-heres-sol...\n",
      "4   HTTPS  ...                                        keppel/pinn\n",
      "5    Http  ...                news/2015-09-scale-solar-youve.html\n",
      "6   https  ...                                                   \n",
      "7    http  ...  2016/04/custom-deleters-for-c-smart-pointers.html\n",
      "8    http  ...                                     ?beta=agnipath\n",
      "9   https  ...                                             ?param\n",
      "10   http  ...                                                   \n",
      "\n",
      "[11 rows x 3 columns]\n",
      "           0  ...                                                  2\n",
      "0       http  ...                                                   \n",
      "1       http  ...  entertainment/2013/04/florida-djs-april-fools-...\n",
      "2      https  ...  Technology-Ventures-Enterprise-Thomas-Byers/dp...\n",
      "3       http  ...                2007/11/07/movies/07stein.html?_r=0\n",
      "4       http  ...  business/2015/10/comcast-and-other-isps-boost-...\n",
      "...      ...  ...                                                ...\n",
      "20094  https  ...  philosophy/how-purism-avoids-intels-active-man...\n",
      "20095  https  ...  @zreitano/the-yc-application-broken-down-and-t...\n",
      "20096   http  ...                           technology/2016/01/01/0/\n",
      "20097  https  ...  @benjiwheeler/how-product-hunt-really-works-d8...\n",
      "20098  https  ...                                 jmcarp/robobrowser\n",
      "\n",
      "[20099 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "pattern=r\"(https?)://([\\w\\.\\-]+)/?(.*)\"\n",
    "\n",
    "test_url_parts=test_urls.str.extract(pattern, flags=re.I, expand=False)\n",
    "\n",
    "print(test_url_parts)\n",
    "\n",
    "url_parts=hn[\"url\"].str.extract(pattern, flags=re.I, expand=False)\n",
    "\n",
    "print(url_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Using Named Capture Groups to Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/dq-content/369/named_capture_groups.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      protocol  ...                                               path\n",
      "0         http  ...                                                   \n",
      "1         http  ...  entertainment/2013/04/florida-djs-april-fools-...\n",
      "2        https  ...  Technology-Ventures-Enterprise-Thomas-Byers/dp...\n",
      "3         http  ...                2007/11/07/movies/07stein.html?_r=0\n",
      "4         http  ...  business/2015/10/comcast-and-other-isps-boost-...\n",
      "...        ...  ...                                                ...\n",
      "20094    https  ...  philosophy/how-purism-avoids-intels-active-man...\n",
      "20095    https  ...  @zreitano/the-yc-application-broken-down-and-t...\n",
      "20096     http  ...                           technology/2016/01/01/0/\n",
      "20097    https  ...  @benjiwheeler/how-product-hunt-really-works-d8...\n",
      "20098    https  ...                                 jmcarp/robobrowser\n",
      "\n",
      "[20099 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(?P<protocol>https?)://(?P<domain>[\\w\\.\\-]+)/?(?P<path>.*)\"\n",
    "\n",
    "url_parts=hn[\"url\"].str.extract(pattern, flags=re.I)\n",
    "\n",
    "print(url_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned advanced regular expression techniques to help us work with text data, including:\n",
    "\n",
    "* Using multiple capture groups to extract URL data.\n",
    "\n",
    "* How to use lookarounds to customize matches based on the surrounding text.\n",
    "\n",
    "* How to substitute a regular expression match to clean inconsistent data.\n",
    "\n",
    "* How to use named capture groups to extract dataframes from a text column.\n",
    "\n",
    "* These techniques allow us to clean and analyze text data in an extremely powerful way, and will be one of the most useful tools in your data-cleaning \"toolbelt\" as you continue on your learning journey.\n",
    "\n",
    "As we mentioned at the outset, unless you find yourself analyzing and cleaning text data with regular expressions regularly, it's unlikely that you'll remember every detail of regex syntax. The key with regular expressions is to understand the key concepts and what is possible, and know where and how to look up the rest.\n",
    "\n",
    "With that in mind, don't be bothered if you don't feel like a regex guru right now - that's totally normal, and you'll start to feel better as you use this new data cleaning tool more and more over time.\n",
    "\n",
    "In the next lesson, we'll learn two powerful techniques to speed up the data cleaning workflow: lambda functions and list comprehension.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
